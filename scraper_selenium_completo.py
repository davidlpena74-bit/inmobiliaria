"""
Scraper Mejorado de Idealista con Selenium
===========================================
Este scraper usa Selenium para obtener TODAS las propiedades disponibles
mediante paginaciÃ³n automÃ¡tica.

CaracterÃ­sticas:
- PaginaciÃ³n automÃ¡tica hasta obtener todas las propiedades
- Manejo de anti-bot con delays aleatorios
- ExtracciÃ³n completa de datos
- ExportaciÃ³n a mÃºltiples formatos
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import pandas as pd
import time
import random
from datetime import datetime
import os
import re


class IdealistaScraperSelenium:
    """
    Scraper robusto con Selenium que obtiene TODAS las propiedades
    """
    
    def __init__(self, zona: str = "vicalvaro", ciudad: str = "madrid", tipo: str = "venta"):
        self.zona = zona
        self.ciudad = ciudad
        self.tipo = tipo  # "venta" o "alquiler"
        self.base_url = f"https://www.idealista.com/{tipo}-viviendas/{ciudad}/{zona}/"
        self.propiedades = []
        self.driver = None
        
    def iniciar_navegador(self, headless: bool = False, usar_proxy: bool = False, 
                          proxy_host: str = None, proxy_port: str = None,
                          proxy_user: str = None, proxy_pass: str = None):
        """
        Inicia el navegador Chrome con opciones anti-detecciÃ³n
        
        Args:
            headless: Ejecutar sin interfaz grÃ¡fica
            usar_proxy: Activar proxy (DataImpulse VPN)
            proxy_host: Host del proxy (ej: "gate.dataimpulse.com")
            proxy_port: Puerto del proxy (ej: "823")
            proxy_user: Usuario del proxy
            proxy_pass: ContraseÃ±a del proxy
        """
        print("ğŸš€ Iniciando navegador...")
        
        chrome_options = Options()
        
        # ========================================
        # OPTIMIZACIÃ“N: BLOQUEAR IMÃGENES
        # ========================================
        # Esto hace el scraping MUCHO mÃ¡s rÃ¡pido y ligero
        prefs = {
            "profile.managed_default_content_settings.images": 2,  # Bloquear imÃ¡genes
            "profile.default_content_setting_values.notifications": 2,  # Bloquear notificaciones
            "profile.managed_default_content_settings.stylesheets": 2,  # Opcional: bloquear CSS
            "profile.managed_default_content_settings.cookies": 1,  # Permitir cookies
            "profile.managed_default_content_settings.javascript": 1,  # Permitir JS
            "profile.managed_default_content_settings.plugins": 2,  # Bloquear plugins
            "profile.managed_default_content_settings.popups": 2,  # Bloquear popups
            "profile.managed_default_content_settings.geolocation": 2,  # Bloquear geolocalizaciÃ³n
            "profile.managed_default_content_settings.media_stream": 2,  # Bloquear media
        }
        chrome_options.add_experimental_option("prefs", prefs)
        print("ğŸš« ImÃ¡genes bloqueadas (acceso ligero activado)")
        
        # ========================================
        # CONFIGURACIÃ“N DE PROXY (DataImpulse)
        # ========================================
        if usar_proxy and proxy_host and proxy_port:
            if proxy_user and proxy_pass:
                # Proxy con autenticaciÃ³n
                proxy_string = f"{proxy_user}:{proxy_pass}@{proxy_host}:{proxy_port}"
                chrome_options.add_argument(f'--proxy-server=http://{proxy_string}')
                print(f"ğŸ”’ Proxy configurado: {proxy_host}:{proxy_port} (con autenticaciÃ³n)")
            else:
                # Proxy sin autenticaciÃ³n
                chrome_options.add_argument(f'--proxy-server=http://{proxy_host}:{proxy_port}')
                print(f"ğŸ”’ Proxy configurado: {proxy_host}:{proxy_port}")
        
        # ========================================
        # ANTI-DETECCIÃ“N
        # ========================================
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User agent realista
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # ========================================
        # OPTIMIZACIONES DE RENDIMIENTO
        # ========================================
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins-discovery')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        
        # Headless mode
        if headless:
            chrome_options.add_argument('--headless=new')  # Nuevo headless mode
            print("ğŸ‘» Modo headless activado")
        
        # Otras opciones
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--lang=es-ES')
        chrome_options.add_argument('--disable-logging')
        chrome_options.add_argument('--log-level=3')
        
        # Iniciar navegador
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # Modificar webdriver property para evitar detecciÃ³n
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # Configurar timeouts
        self.driver.set_page_load_timeout(30)
        self.driver.implicitly_wait(5)
        
        print("âœ… Navegador iniciado correctamente")
        
    def delay_aleatorio(self, min_sec: float = 2.0, max_sec: float = 5.0):
        """Espera aleatoria entre acciones"""
        tiempo = random.uniform(min_sec, max_sec)
        print(f"â³ Esperando {tiempo:.2f} segundos...")
        time.sleep(tiempo)
    
    def scroll_suave(self):
        """Hace scroll suave por la pÃ¡gina para simular comportamiento humano"""
        total_height = self.driver.execute_script("return document.body.scrollHeight")
        for i in range(1, 10):
            scroll_to = (total_height / 10) * i
            self.driver.execute_script(f"window.scrollTo(0, {scroll_to});")
            time.sleep(random.uniform(0.2, 0.5))
    
    def obtener_numero_total_propiedades(self):
        """Extrae el nÃºmero total de propiedades disponibles"""
        try:
            # Idealista muestra el total en diferentes lugares, intentamos varios selectores
            selectores = [
                "h1",
                ".listing-title",
                ".main-info__title-main",
                "[data-testid='total-results']"
            ]
            
            for selector in selectores:
                try:
                    elemento = self.driver.find_element(By.CSS_SELECTOR, selector)
                    texto = elemento.text
                    
                    # Buscar nÃºmeros en el texto (ej: "345 viviendas en venta")
                    numeros = re.findall(r'\d+', texto.replace('.', ''))
                    if numeros:
                        total = int(numeros[0])
                        print(f"ğŸ“Š Total de propiedades encontradas: {total}")
                        return total
                except:
                    continue
            
            print("âš ï¸ No se pudo determinar el nÃºmero total de propiedades")
            return None
            
        except Exception as e:
            print(f"âš ï¸ Error obteniendo total: {e}")
            return None
    
    def extraer_propiedades_pagina(self):
        """Extrae todas las propiedades de la pÃ¡gina actual"""
        propiedades_pagina = []
        
        try:
            # Esperar a que carguen los items
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "article.item"))
            )
            
            # Scroll suave para cargar todo
            self.scroll_suave()
            
            # Encontrar todos los artÃ­culos de propiedades
            items = self.driver.find_elements(By.CSS_SELECTOR, "article.item")
            print(f"ğŸ  Encontrados {len(items)} anuncios en esta pÃ¡gina")
            
            for item in items:
                try:
                    propiedad = self._extraer_datos_item(item)
                    if propiedad:
                        propiedades_pagina.append(propiedad)
                except Exception as e:
                    print(f"âš ï¸ Error extrayendo propiedad: {e}")
                    continue
            
        except TimeoutException:
            print("âš ï¸ Timeout esperando propiedades")
        except Exception as e:
            print(f"âŒ Error: {e}")
        
        return propiedades_pagina
    
    def _extraer_datos_item(self, item):
        """Extrae datos de un item individual"""
        propiedad = {}
        
        try:
            # Precio
            try:
                precio_elem = item.find_element(By.CSS_SELECTOR, ".item-price")
                precio_text = precio_elem.text.strip()
                # Limpiar: "215.000 â‚¬" -> 215000
                precio_limpio = re.sub(r'[^\d]', '', precio_text)
                if precio_limpio:
                    propiedad['precio'] = int(precio_limpio)
            except:
                pass
            
            # TÃ­tulo y URL
            try:
                titulo_elem = item.find_element(By.CSS_SELECTOR, "a.item-link")
                propiedad['titulo'] = titulo_elem.text.strip()
                propiedad['url'] = titulo_elem.get_attribute('href')
            except:
                pass
            
            # CaracterÃ­sticas (mÂ², habitaciones, baÃ±os)
            try:
                detalles = item.find_elements(By.CSS_SELECTOR, ".item-detail")
                detalles_text = " ".join([d.text for d in detalles])
                propiedad['detalles'] = detalles_text
                
                # Extraer mÂ²
                m2_match = re.search(r'(\d+)\s*mÂ²', detalles_text)
                if m2_match:
                    propiedad['m2'] = int(m2_match.group(1))
                
                # Extraer habitaciones
                hab_match = re.search(r'(\d+)\s*hab', detalles_text)
                if hab_match:
                    propiedad['habitaciones'] = int(hab_match.group(1))
                
                # Extraer baÃ±os
                bano_match = re.search(r'(\d+)\s*baÃ±o', detalles_text)
                if bano_match:
                    propiedad['banos'] = int(bano_match.group(1))
                    
            except:
                pass
            
            # DescripciÃ³n
            try:
                desc_elem = item.find_element(By.CSS_SELECTOR, ".item-description")
                propiedad['descripcion'] = desc_elem.text.strip()
            except:
                pass
            
            # Calcular precio/mÂ²
            if 'precio' in propiedad and 'm2' in propiedad and propiedad['m2'] > 0:
                propiedad['precio_m2'] = round(propiedad['precio'] / propiedad['m2'], 2)
            
            # Metadata
            propiedad['zona'] = self.zona.title()
            propiedad['ciudad'] = self.ciudad.title()
            propiedad['tipo'] = self.tipo
            propiedad['fecha_extraccion'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            return propiedad if propiedad else None
            
        except Exception as e:
            return None
    
    def ir_siguiente_pagina(self):
        """Navega a la siguiente pÃ¡gina de resultados"""
        try:
            # Buscar botÃ³n de siguiente pÃ¡gina
            siguiente = self.driver.find_element(By.CSS_SELECTOR, ".icon-arrow-right-after")
            
            # Scroll hasta el botÃ³n
            self.driver.execute_script("arguments[0].scrollIntoView();", siguiente)
            time.sleep(1)
            
            # Hacer clic
            siguiente.click()
            print("â¡ï¸ Navegando a siguiente pÃ¡gina...")
            
            # Esperar a que cargue la nueva pÃ¡gina
            time.sleep(random.uniform(3, 5))
            return True
            
        except NoSuchElementException:
            print("âœ… No hay mÃ¡s pÃ¡ginas (Ãºltima pÃ¡gina alcanzada)")
            return False
        except Exception as e:
            print(f"âš ï¸ Error navegando a siguiente pÃ¡gina: {e}")
            return False
    
    def scrape_todas_las_paginas(self, max_paginas: int = None, headless: bool = False,
                                  usar_proxy: bool = False, proxy_host: str = None,
                                  proxy_port: str = None, proxy_user: str = None,
                                  proxy_pass: str = None):
        """
        Scrape todas las pÃ¡ginas disponibles
        
        Args:
            max_paginas: LÃ­mite de pÃ¡ginas (None = todas)
            headless: Ejecutar sin interfaz grÃ¡fica
            usar_proxy: Activar proxy DataImpulse
            proxy_host: Host del proxy
            proxy_port: Puerto del proxy
            proxy_user: Usuario del proxy
            proxy_pass: ContraseÃ±a del proxy
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ Iniciando scraping completo de Idealista")
        print(f"ğŸ“ Zona: {self.zona.title()}, {self.ciudad.title()}")
        print(f"ğŸ“„ Tipo: {self.tipo.title()}")
        if max_paginas:
            print(f"ğŸ“„ LÃ­mite de pÃ¡ginas: {max_paginas}")
        else:
            print(f"ğŸ“„ Scrapeando TODAS las pÃ¡ginas disponibles")
        print(f"{'='*60}\n")
        
        try:
            # Iniciar navegador con configuraciÃ³n
            self.iniciar_navegador(
                headless=headless,
                usar_proxy=usar_proxy,
                proxy_host=proxy_host,
                proxy_port=proxy_port,
                proxy_user=proxy_user,
                proxy_pass=proxy_pass
            )
            
            # Ir a la primera pÃ¡gina
            print(f"ğŸŒ Navegando a: {self.base_url}")
            self.driver.get(self.base_url)
            self.delay_aleatorio(3, 5)
            
            # Obtener total de propiedades
            total_esperado = self.obtener_numero_total_propiedades()
            
            # Aceptar cookies si aparecen
            try:
                cookie_btn = self.driver.find_element(By.ID, "didomi-notice-agree-button")
                cookie_btn.click()
                print("ğŸª Cookies aceptadas")
                time.sleep(1)
            except:
                pass
            
            # Scrapear pÃ¡ginas
            pagina = 1
            while True:
                print(f"\n--- PÃ¡gina {pagina} ---")
                
                # Extraer propiedades de la pÃ¡gina actual
                props_pagina = self.extraer_propiedades_pagina()
                self.propiedades.extend(props_pagina)
                
                print(f"âœ… ExtraÃ­das {len(props_pagina)} propiedades")
                print(f"ğŸ“Š Total acumulado: {len(self.propiedades)} propiedades")
                
                # Verificar si alcanzamos el lÃ­mite
                if max_paginas and pagina >= max_paginas:
                    print(f"\nâš ï¸ LÃ­mite de {max_paginas} pÃ¡ginas alcanzado")
                    break
                
                # Intentar ir a siguiente pÃ¡gina
                if not self.ir_siguiente_pagina():
                    break
                
                pagina += 1
                self.delay_aleatorio(4, 7)  # Delay mÃ¡s largo entre pÃ¡ginas
            
            print(f"\n{'='*60}")
            print(f"âœ… Scraping completado")
            print(f"ğŸ“Š Total de propiedades extraÃ­das: {len(self.propiedades)}")
            if total_esperado:
                print(f"ğŸ“Š Total esperado: {total_esperado}")
                cobertura = (len(self.propiedades) / total_esperado) * 100
                print(f"ğŸ“Š Cobertura: {cobertura:.1f}%")
            print(f"{'='*60}\n")
            
        except Exception as e:
            print(f"âŒ Error durante el scraping: {e}")
        
        finally:
            # Cerrar navegador
            if self.driver:
                self.driver.quit()
                print("ğŸ”’ Navegador cerrado")
        
        # Convertir a DataFrame
        if self.propiedades:
            return pd.DataFrame(self.propiedades)
        else:
            return pd.DataFrame()
    
    def guardar_datos(self, df: pd.DataFrame, formato: str = 'csv'):
        """Guarda los datos en diferentes formatos"""
        if df.empty:
            print("âš ï¸ No hay datos para guardar")
            return
        
        os.makedirs('datos', exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_filename = f"datos/idealista_{self.zona}_{self.tipo}_{timestamp}"
        
        if formato == 'csv':
            filename = f"{base_filename}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ Datos guardados en: {filename}")
        
        elif formato == 'json':
            filename = f"{base_filename}.json"
            df.to_json(filename, orient='records', force_ascii=False, indent=2)
            print(f"ğŸ’¾ Datos guardados en: {filename}")
        
        elif formato == 'excel':
            filename = f"{base_filename}.xlsx"
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"ğŸ’¾ Datos guardados en: {filename}")
        
        return filename


def main():
    """FunciÃ³n principal"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘      SCRAPER COMPLETO DE IDEALISTA CON SELENIUM            â•‘
    â•‘                                                            â•‘
    â•‘  ğŸ¯ Obtiene TODAS las propiedades mediante paginaciÃ³n      â•‘
    â•‘  ğŸ“Š Ejemplo: 345 propiedades en VicÃ¡lvaro                  â•‘
    â•‘  ğŸš« ImÃ¡genes bloqueadas (acceso ligero)                    â•‘
    â•‘  ğŸ”’ Soporte para proxy DataImpulse                         â•‘
    â•‘  âš ï¸  Uso exclusivamente acadÃ©mico                          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Intentar importar configuraciÃ³n
    try:
        from config import (
            ZONA, CIUDAD, TIPO, MAX_PAGINAS, HEADLESS,
            USAR_PROXY, PROXY_HOST, PROXY_PORT, PROXY_USER_BASE, PROXY_PASS
        )
        print("âœ… ConfiguraciÃ³n cargada desde config.py")
    except ImportError:
        # ConfiguraciÃ³n por defecto si no existe config.py
        print("âš ï¸ No se encontrÃ³ config.py, usando configuraciÃ³n por defecto")
        ZONA = "vicalvaro"
        CIUDAD = "madrid"
        TIPO = "venta"
        MAX_PAGINAS = None
        HEADLESS = False
        USAR_PROXY = False
        PROXY_HOST = None
        PROXY_PORT = None
        PROXY_USER = None
        PROXY_USER_BASE = None
        PROXY_PASS = None
    
    # Generar usuario Proxy con sesiÃ³n aleatoria si es necesario
    PROXY_USER = None
    if USAR_PROXY and PROXY_USER_BASE:
        session_id = random.randint(100000, 999999)
        PROXY_USER = f"{PROXY_USER_BASE}__country-es__session-{session_id}"
        print(f"ğŸŒ IP Rotada - Nueva SesiÃ³n: {session_id}")

    # Mostrar configuraciÃ³n
    print(f"\nğŸ“‹ CONFIGURACIÃ“N:")
    print(f"   Zona: {ZONA}")
    print(f"   Ciudad: {CIUDAD}")
    print(f"   Tipo: {TIPO}")
    print(f"   PÃ¡ginas: {'Todas' if MAX_PAGINAS is None else MAX_PAGINAS}")
    print(f"   Headless: {'SÃ­' if HEADLESS else 'No'}")
    print(f"   Proxy: {'Activado âœ…' if USAR_PROXY else 'Desactivado'}")
    if USAR_PROXY:
        print(f"   Proxy Host: {PROXY_HOST}:{PROXY_PORT}")
    print()
    
    # Crear scraper
    scraper = IdealistaScraperSelenium(zona=ZONA, ciudad=CIUDAD, tipo=TIPO)
    
    # Ejecutar scraping completo
    df = scraper.scrape_todas_las_paginas(
        max_paginas=MAX_PAGINAS,
        headless=HEADLESS,
        usar_proxy=USAR_PROXY,
        proxy_host=PROXY_HOST,
        proxy_port=PROXY_PORT,
        proxy_user=PROXY_USER,
        proxy_pass=PROXY_PASS
    )
    
    # Mostrar resultados
    if not df.empty:
        print("\nğŸ“Š RESUMEN DE DATOS EXTRAÃDOS:")
        print(f"   Total propiedades: {len(df)}")
        
        if 'precio' in df.columns:
            precios_numericos = pd.to_numeric(df['precio'], errors='coerce')
            print(f"   Precio medio: {precios_numericos.mean():,.0f} â‚¬")
            print(f"   Precio mÃ­nimo: {precios_numericos.min():,.0f} â‚¬")
            print(f"   Precio mÃ¡ximo: {precios_numericos.max():,.0f} â‚¬")
        
        if 'precio_m2' in df.columns:
            print(f"   Precio/mÂ² medio: {df['precio_m2'].mean():,.2f} â‚¬/mÂ²")
        
        if 'm2' in df.columns:
            print(f"   mÂ² medio: {df['m2'].mean():.0f} mÂ²")
        
        if 'habitaciones' in df.columns:
            print(f"   Habitaciones media: {df['habitaciones'].mean():.1f}")
        
        # Guardar datos
        print("\nğŸ’¾ Guardando datos...")
        scraper.guardar_datos(df, formato='csv')
        scraper.guardar_datos(df, formato='json')
        scraper.guardar_datos(df, formato='excel')
        
        # Mostrar muestra
        print("\nğŸ“‹ MUESTRA DE DATOS (primeras 5 propiedades):")
        columnas = ['titulo', 'precio', 'm2', 'precio_m2', 'habitaciones']
        columnas_disponibles = [c for c in columnas if c in df.columns]
        print(df[columnas_disponibles].head(5).to_string(index=False))
        
    else:
        print("\nâš ï¸ No se pudieron extraer datos")


if __name__ == "__main__":
    main()
